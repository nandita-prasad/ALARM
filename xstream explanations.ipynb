{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C38cYmPfvUw_"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZmuCIJzyOG1"
      },
      "outputs": [],
      "source": [
        "!wget -q https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkAyV1AbyVeC"
      },
      "outputs": [],
      "source": [
        "!tar xf spark-3.1.2-bin-hadoop2.7.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9z-pI-wkv9QR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvFkcMSizT5T"
      },
      "outputs": [],
      "source": [
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P12uZ3YnzWY0"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKjyBKerzb_m",
        "outputId": "5dd326d8-210f-4aa0-aafc-3041e8ea48e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num executors: None\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "spark = SparkSession(sc)\n",
        "\n",
        "#spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "print(f'num executors: {sc.getConf().get(\"spark.executor.instances\")}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qick8111tp0",
        "outputId": "67a40ee0-629f-4959-e85b-9140eaa832f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |██████▍                         | 10 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 20 kB 18.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 30 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 40 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 50 kB 2.9 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q mmh3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3Ok-Tfd1n3D"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "import mmh3\n",
        "import pandas as pd\n",
        "import tqdm\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
        "\n",
        "np.set_printoptions(threshold=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-3LGBo23JmJ",
        "outputId": "ca3aee15-414f-4122-d05c-8c60e7b7d894"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g86BPodEU9Uo"
      },
      "source": [
        "##Import Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37wHPriGUncW",
        "outputId": "0e3fbabf-f00c-469c-afe1-a66cb032c1cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running xStream\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "print(\"Running xStream\")\n",
        "print(sc.defaultParallelism)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gsb-USCusHfL"
      },
      "source": [
        "## Exec: Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhJWAdN9sH6Z"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vla-r-roFlY"
      },
      "outputs": [],
      "source": [
        "projection=True\n",
        "isMutable=True #set to True if mixed-attributes\n",
        "projdim = 30 #K\n",
        "nchains = 50\n",
        "depth = 15\n",
        "\n",
        "r = 3\n",
        "w = 50\n",
        "\n",
        "numPartitions = 64\n",
        "nthreads_fit = 128\n",
        "nthreads_score = 128\n",
        "samplerate =  1 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kzMddY-zXZz"
      },
      "outputs": [],
      "source": [
        "part = 99\n",
        "sys_config = 7\n",
        "model_file = \"model_\" + str(nchains) + \"_\" + str(depth) + \".pkl\"\n",
        "#output_file = \"output_score_\" + str(nchains) + \"_\" + str(depth) + \"_\" + str(int(samplerate * 100)) + \"_\" + str(sys_config) + \".txt\"\n",
        "output_file = \"output_f1.txt\"\n",
        "score_folder = \"score_\" + str(nchains) + \"_\" + str(depth) + \"_\" + str(int(samplerate * 100)) + \"_\" + str(sys_config) +\".csv\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zb-TkiODVUkb"
      },
      "outputs": [],
      "source": [
        "# splitting file\n",
        "split = False\n",
        "if split:\n",
        "    df = pd.read_csv('/ocean/projects/ccr190032p/shared/osm_data/simple-gps-combined-1-with-labels.txt', sep=',', header=None, error_bad_lines=False)\n",
        "    count = df.shape[0]\n",
        "    split_size = 10\n",
        "    df_1 = df.iloc[:int(count/split_size),:]\n",
        "    df_2 = df.iloc[int(count/split_size):int(2*count/split_size),:]\n",
        "    df_3 = df.iloc[int(2*count/split_size):int(3*count/split_size),:]\n",
        "    df_4 = df.iloc[int(3*count/split_size):int(4*count/split_size),:]\n",
        "    df_5 = df.iloc[int(4*count/split_size):int(5*count/split_size),:]\n",
        "    df_6 = df.iloc[int(5*count/split_size):int(6*count/split_size),:]\n",
        "    df_7 = df.iloc[int(6*count/split_size):int(7*count/split_size),:]\n",
        "    df_8 = df.iloc[int(7*count/split_size):int(8*count/split_size),:]\n",
        "    df_9 = df.iloc[int(8*count/split_size):int(9*count/split_size),:]\n",
        "    df_10 = df.iloc[int(9*count/split_size):,:]\n",
        "    \n",
        "    df_1.to_csv(\"/ocean/projects/cie170025p/shared/osm_10/osm_all_1.txt\", header=None, index=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_Jg-4ZPVZJJ",
        "outputId": "485bfbe8-28dd-4ab1-ea0e-0c7397323122"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Row(_c0='Male', _c1='Bulgaria', _c2=34, _c3=100, _c4=0), Row(_c0='Male', _c1='Bulgaria', _c2=36, _c3=140, _c4=0), Row(_c0='Male', _c1='India', _c2=32, _c3=120, _c4=0), Row(_c0='Female', _c1='Turkey', _c2=33, _c3=20, _c4=1), Row(_c0='Male', _c1='Germany', _c2=45, _c3=75, _c4=0)]\n",
            "Read time = 10.464s\n",
            "1\n",
            "(7, 5)\n"
          ]
        }
      ],
      "source": [
        "test_big = True\n",
        "if test_big:\n",
        "    start = time.time()\n",
        "    #myDF = spark.read.options(inferSchema='True').csv(\"synthData-10m.csv\")\n",
        "    #myDF = spark.read.options(inferSchema='True').csv(\"/ocean/projects/ccr190032p/shared/synthData-big/ionosphere_5m.X.csv\")\n",
        "    #myDF = spark.read.options(inferSchema='True').csv(\"test-data.csv\")\n",
        "    #myDF = spark.read.options(inferSchema='True').csv(\"synDataNoisy_all.csv\")\n",
        "    #myDF = spark.read.options(inferSchema='True').csv(\"/ocean/projects/ccr190032p/shared/synthData-big/gisette_40k_3.all.csv\")\n",
        "    #myDF = spark.read.options(inferSchema='True').csv(\"/ocean/projects/ccr190032p/shared/osm_data/simple-gps-combined-1-with-labels.txt\")\n",
        "    #myDF = spark.read.options(inferSchema='True').csv(\"/ocean/projects/ccr190032p/shared/Spam_URL/SpamURL_100_labels.csv\")\n",
        " \n",
        "    myDF = spark.read.options(inferSchema='True').csv(\"/content/drive/My\\ Drive/ColabNotebooks/data/test-data-x.csv\")\n",
        "    #myDF = spark.read.options(inferSchema='True').csv(\"/content/drive/My\\ Drive/ColabNotebooks/data/test-data-ohe-x.csv\")\n",
        "    \n",
        "    myDF = myDF.na.fill('').na.fill(0)\n",
        "    \n",
        "    end = time.time()\n",
        "    count = myDF.count()\n",
        "    print(myDF.take(5))\n",
        "    print(\"Read time = %.3fs\" % (end - start))\n",
        "    print(myDF.rdd.getNumPartitions())\n",
        "    print((count, len(myDF.columns)))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkTs5d-e4CP-",
        "outputId": "1a90e119-46f1-4800-8bc8-b191e2fbf44b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _c0: string (nullable = false)\n",
            " |-- _c1: string (nullable = false)\n",
            " |-- _c2: integer (nullable = true)\n",
            " |-- _c3: integer (nullable = true)\n",
            " |-- _c4: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "myDF.printSchema()\n",
        "#myDF.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWV9yDM9zTbc",
        "outputId": "644240cc-eece-44d7-8ca3-f88eb529c12e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repartition: 64\n",
            "(7, 5)\n"
          ]
        }
      ],
      "source": [
        "repartition = True\n",
        "if repartition:\n",
        "    myDF = myDF.repartition(numPartitions)\n",
        "    print(\"Repartition:\", myDF.rdd.getNumPartitions())\n",
        "print((myDF.count(), len(myDF.columns)))\n",
        "    \n",
        "#y = np.array(myDF.select(myDF.columns[-1]).collect())\n",
        "#y=y.reshape(-1)\n",
        "#print(y.shape)\n",
        "#myDF = myDF.select(myDF.columns[0:-1])\n",
        "#myDF_new = myDF.rdd.map(lambda x: (list(x[0:-1]), x[-1]))\n",
        "#myDF_new.count()\n",
        "all_columns = myDF.columns[:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kn0MMz8iWun9"
      },
      "outputs": [],
      "source": [
        "downsize = False\n",
        "factor = 0.0000001\n",
        "if downsize:\n",
        "    myDF = myDF.limit(int(count * factor))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIphwiPxVDR7"
      },
      "source": [
        "##Min-Max Normalize Numerical Features Prior to Projection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdRau_Os4owQ"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import max as sparkMax\n",
        "from pyspark.sql.functions import min as sparkMin\n",
        "\n",
        "def min_max_normalize(df):\n",
        "    feature_names = all_columns\n",
        "    print(feature_names)\n",
        "    # getting the max and min for all columns\n",
        "    max_df = df.select(*[sparkMax(col(c)).alias(c) for c in feature_names])\n",
        "    all_max = max_df.toPandas().to_numpy()[0]\n",
        "    min_df = df.select(*[sparkMin(col(c)).alias(c) for c in feature_names])\n",
        "    all_min = min_df.toPandas().to_numpy()[0]\n",
        "\n",
        "    #print(all_max)\n",
        "    for i in range(len(feature_names)): # <-- THREADS?\n",
        "        if ((df.dtypes[i][1]) == 'string'):\n",
        "            continue\n",
        "        #min_val = df.select(feature_names[i]).rdd.reduce(lambda a, b: a if a < b else b)[0]\n",
        "        #max_val = df.select(feature_names[i]).rdd.reduce(lambda a, b: a if a > b else b)[0]\n",
        "        #min_val = df.agg({feature_names[i]: \"min\"}).collect()[0][0]\n",
        "        #max_val = df.agg({feature_names[i]: \"max\"}).collect()[0][0]\n",
        "        min_val = all_min[i] #.item()\n",
        "        max_val = all_max[i] #.item()\n",
        "        \n",
        "        if (min_val == 0 and max_val == 1):\n",
        "           continue\n",
        "        print(feature_names[i])\n",
        "        df = df.withColumn(feature_names[i],(col(feature_names[i]) - min_val) / (max_val - min_val))\n",
        "        # min-max takes >50x time compared to normalize\n",
        "        # IS THIS <withColumn> LOCAL?  \n",
        "    return df # <-- IS THIS RETURNING/COLLECTING TO THE DRIVER?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgJJhMt_5cbf"
      },
      "outputs": [],
      "source": [
        "# all_outputs = []\n",
        "# min_max = True\n",
        "# if min_max:\n",
        "#     print(\"Min-max normalize\")\n",
        "#     all_outputs.append(\"Min-max normalize\")\n",
        "#     #start = time.time()\n",
        "#     normDF = min_max_normalize(myDF)\n",
        "#     #normDF.show()\n",
        "#     #end = time.time()\n",
        "#     #print(\"Time = %.3fs\" % (end-start))\n",
        "#     #all_outputs.append(\"Time = %.3fs\" % (end-start))\n",
        "# else:\n",
        "#     normDF = myDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAsxUGFH-XoL"
      },
      "outputs": [],
      "source": [
        "class StreamhashProjection:\n",
        "\n",
        "    def __init__(self, n_components, feature_names=None, density=1/3.0, random_state=None, mutable=False, sc=None):\n",
        "        self.keys = np.arange(0, n_components, 1)\n",
        "        self.constant = np.sqrt(1./density)/np.sqrt(n_components)\n",
        "        self.density = density\n",
        "        self.n_components = n_components\n",
        "        self.mutable=mutable\n",
        "        random.seed(random_state)\n",
        "        \n",
        "        input_dim = len(feature_names)\n",
        "        self.R = sc.broadcast(np.array([[self._hash_string(k, f) # shape: (len(self.keys), input_dim)\n",
        "                       for f in feature_names]\n",
        "                       for k in self.keys]))\n",
        "\n",
        "    def fit_transform(self, X, feature_names=None): \n",
        "\n",
        "        R = self.R.value  \n",
        "          \n",
        "        if self.mutable:\n",
        "            #nsamples = 1      # X.shape[0]\n",
        "            ndim = len(X)     # X.shape[1]\n",
        "            if feature_names is None:\n",
        "                feature_names = [str(i) for i in range(ndim)]\n",
        "\n",
        "            types = [type(X[i]) == str for i in range(ndim)] #!!! [0][i] not [i]\n",
        "\n",
        "            feature_names = [feature_names[i]+'.'+X[i] if types[i] else feature_names[i] for i in range(ndim)]\n",
        "            X = [1 if types[i] else X[i] for i in range(ndim)]\n",
        "\n",
        "            \n",
        "            for i in range(ndim):\n",
        "                if types[i]:\n",
        "                    f = feature_names[i]\n",
        "                    R[:,i] = np.array([self._hash_string(k, f) for k in self.keys])\n",
        "                \n",
        "            Y = np.dot(X, R.T)\n",
        "        else:\n",
        "            Y = np.dot(X, self.R.T)\n",
        "        \n",
        "        return Y\n",
        "\n",
        "    def explain_transform(self, X, feature_names=None): \n",
        "\n",
        "        R = self.R.value  \n",
        "          \n",
        "        if self.mutable:\n",
        "            #nsamples = 1      # X.shape[0]\n",
        "            ndim = len(X)     # X.shape[1]\n",
        "            if feature_names is None:\n",
        "                feature_names = [str(i) for i in range(ndim)]\n",
        "\n",
        "            types = [type(X[i]) == str for i in range(ndim)] #!!! [0][i] not [i]\n",
        "\n",
        "            feature_names = [feature_names[i]+'.'+X[i] if types[i] else feature_names[i] for i in range(ndim)]\n",
        "            #X = [1 if types[i] else X[i] for i in range(ndim)]\n",
        "\n",
        "            \n",
        "            for i in range(ndim):\n",
        "                if types[i]:\n",
        "                    f = feature_names[i]\n",
        "                    R[:,i] = np.array([self._hash_string(k, f) for k in self.keys])\n",
        "                \n",
        "            #Y = np.dot(X, R.T)\n",
        "        #else:\n",
        "            #Y = np.dot(X, self.R.T)\n",
        "        \n",
        "        return R\n",
        "        \n",
        "    def explain_transform_ohe(self, allX, feature_names=None): \n",
        "\n",
        "        #numunique is a ndim-dimensional vector containing number of unique values per feature\n",
        "     \n",
        "        R = self.R.value\n",
        "\n",
        "        if self.mutable:\n",
        "            if feature_names is None:\n",
        "                feature_names = [str(i) for i in range(ndim)]\n",
        "\n",
        "            ndim = len(feature_names)\n",
        "\n",
        "            X = allX.take(1)\n",
        "            #print(X)\n",
        "            types = [type(X[0][i]) == str for i in range(ndim)]\n",
        "            #print(types)\n",
        "            # find total # of features after OHE\n",
        "            num_unique = np.zeros((ndim,1))\n",
        "            for i in range(ndim):\n",
        "                if types[i]:\n",
        "                  num_unique[i] = allX.select(allX.columns[i]).distinct().count()\n",
        "                else:\n",
        "                  num_unique[i] = 1\n",
        "\n",
        "            #print(num_unique)\n",
        "\n",
        "            totalAfterOHE = num_unique.sum()\n",
        "            \n",
        "            R_ohe = np.zeros((self.n_components, int(totalAfterOHE))) \n",
        "            \n",
        "            #feature_names = [feature_names[i]+'.'+X[i] if types[i] else feature_names[i] for i in range(ndim)]\n",
        "     \n",
        "            ind = 0\n",
        "            for i in range(ndim): # for each original feature\n",
        "                if types[i]:\n",
        "                    # find all distinct values\n",
        "                    distinctDF = allX.select(allX.columns[i]).distinct()\n",
        "                    outrdd = distinctDF.rdd.map(lambda x: (x[0], np.array([self._hash_string(k, feature_names[i]+'.'+x[0]) for k in self.keys])))\n",
        "                    for v in outrdd.collect():\n",
        "                        R_ohe[:,ind] = v[1]\n",
        "                        ind = ind+1\n",
        "                else:\n",
        "                  f = feature_names[i]\n",
        "                  R_ohe[:,ind] = np.array([self._hash_string(k, f) for k in self.keys])\n",
        "                  ind = ind+1 # increment by 1 as just 1 ohe feature\n",
        "\n",
        "            return R_ohe\n",
        "            \n",
        "        else:\n",
        "            return R\n",
        "\n",
        "\n",
        "    def _hash_string(self, k, s):\n",
        "        hash_value = int(mmh3.hash(s, signed=False, seed=k))/(2.0**32-1)\n",
        "        den = self.density\n",
        "        if hash_value <= den/2.0:\n",
        "            return self.constant # -1 *\n",
        "        elif hash_value <= den:\n",
        "            return self.constant\n",
        "        else:\n",
        "            return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuAhBYZM-cQw"
      },
      "outputs": [],
      "source": [
        "def projectDF(df, projdim, isMutable, sc=None):\n",
        "    feature_names = all_columns\n",
        "    #for i in range(len(feature_names)):\n",
        "    #  print(feature_names[i])\n",
        "    projector = StreamhashProjection(n_components=projdim, \n",
        "                                     feature_names=feature_names,\n",
        "                                     density=1/3.0, \n",
        "                                     random_state=42, \n",
        "                                     mutable=isMutable,\n",
        "                                     sc=sc)\n",
        "    projectedDF = df.rdd.map(lambda x: projector.fit_transform(list(x[0:-1]),feature_names).tolist() + [x[-1]]).toDF()  \n",
        "\n",
        "    return projectedDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ns0gAS7xjoFz"
      },
      "outputs": [],
      "source": [
        "def feature_range(df):\n",
        "    features = df.columns[:-1]\n",
        "    \"\"\"a = time.time()\n",
        "    max_df = df.select(*[sparkMax(col(c)).alias(c) for c in features])\n",
        "    all_max = max_df.toPandas().to_numpy()[0]\n",
        "    a2 = time.time()\n",
        "    print(a2-a)\n",
        "    min_df = df.select(*[sparkMin(col(c)).alias(c) for c in features])\n",
        "    all_min = min_df.toPandas().to_numpy()[0]\n",
        "    detamax = all_max - all_min\n",
        "    b = time.time()\n",
        "    print(b-a2)\"\"\"\n",
        "    deltamax = np.zeros(len(features), dtype=np.float)  \n",
        "    for f in range(len(features)):\n",
        "        #deltamax = [(df.select(f).rdd.reduce(lambda a, b: a if a > b else b)[0] - df.select(f).rdd.reduce(lambda a, b: a if a < b else b)[0])/2.0 for f in features]\n",
        "        #fRDD = df.rdd.map(lambda x: x[f])\n",
        "        #max_val_f = fRDD.reduce(lambda a, b: a if a > b else b)\n",
        "        #min_val_f = fRDD.reduce(lambda a, b: a if a < b else b)\n",
        "        #deltamax[f] = max_val_f - min_val_f\n",
        "        #deltamax[f] = df.select(f).rdd.max()[0][0] - df.select(f).rdd.min()[0][0]\n",
        "        deltamax[f] = (df.agg({features[f]: \"max\"}).collect()[0][0] - df.agg({features[f]: \"min\"}).collect()[0][0])/2\n",
        "    c = time.time()\n",
        "    #print(c-b)\n",
        "    return deltamax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xz5C1NxFVOSj"
      },
      "source": [
        "## Project the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsd299ee_FzL"
      },
      "outputs": [],
      "source": [
        "    # print(\"Projection\")\n",
        "    # all_outputs.append(\"Projection\")\n",
        "    # start = time.time()\n",
        "    # if projection:\n",
        "    #     pDF = projectDF(normDF, projdim, isMutable, sc=sc)\n",
        "    # else:\n",
        "    #     pDF = normDF\n",
        "      \n",
        "    # deltamax = feature_range(pDF)\n",
        "    # #print(deltamax)\n",
        "    # #### convert into pair rdd ####\n",
        "    # pDF_pair = pDF.rdd.map(lambda x: (list(x[0:-1]), x[-1]))\n",
        "    # pDF_pair.cache()\n",
        "    # end = time.time()\n",
        "    # print(\"Time = %.3fs\" % (end-start))\n",
        "    # all_outputs.append(\"Time = %.3fs\" % (end-start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsAmoICVzrPo"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "from array import array\n",
        "\n",
        "class CMS:\n",
        "    def __init__(self, r, w):\n",
        "        self.r = r\n",
        "        self.w = w\n",
        "    \n",
        "        upper_bound = 2147483647\n",
        "        step = upper_bound / (r-1)\n",
        "        manyranges = [(i*step, step*(i+1)-1) for i in range(r-1)]\n",
        "        self.mask = array('L', (np.random.randint(low, high) for low, high in manyranges))\n",
        "\n",
        "    def findAllRowCols(self, myL):          \n",
        "        h = hash(str(myL)) % self.w\n",
        "\n",
        "        result = []\n",
        "        result.append( (1, h) )\n",
        "        row=2\n",
        "        for m in self.mask:\n",
        "            result.append( (row, (h ^ m) % self.w ))\n",
        "            row+=1\n",
        "        return result\n",
        "\n",
        "    def allCols(self, X):\n",
        "        h = hash(str(X)) % self.w\n",
        "\n",
        "        result = []          \n",
        "        result.append( ((1, h), 1) )\n",
        "        row=2\n",
        "        for m in self.mask:\n",
        "            result.append( ((row, (h ^ m) % self.w), 1) )\n",
        "            row+=1\n",
        "        return result\n",
        "\n",
        "cms = CMS(r=r, w=w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6grLZBrjazQ"
      },
      "outputs": [],
      "source": [
        "class Chain:\n",
        "  \n",
        "    def __init__(self, depth, deltamax):   # depth = 6\n",
        "        k = len(deltamax)\n",
        "        self.deltamax = deltamax # feature ranges\n",
        "        self.depth = depth\n",
        "        self.fs = [np.random.randint(0, k) for d in range(depth)]\n",
        "        print(self.fs)\n",
        "        ### self.cmsketches = [{}] * depth #self.cmsketches = [None] * depth\n",
        "        #???????# does this become a common data structure\n",
        "        self.shift = np.random.rand(k) * deltamax\n",
        "\n",
        "    # input X is a **single** point \n",
        "    def fit(self, X, verbose=False): #!!! all depths\n",
        "        \n",
        "        # initialize cmsketch tables\n",
        "        prebin = np.zeros(len(X), dtype=np.float)\n",
        "        depthcount = np.zeros(len(self.deltamax), dtype=np.int)\n",
        "        \n",
        "        ls = [None] * depth\n",
        "        for d in range(self.depth):\n",
        "          f = self.fs[d] #split feature at depth d\n",
        "          depthcount[f] += 1\n",
        "\n",
        "          if depthcount[f] == 1:\n",
        "              prebin[f] = (X[f] + self.shift[f])/self.deltamax[f] \n",
        "          else:\n",
        "              prebin[f] = 2.0*prebin[f] - self.shift[f]/self.deltamax[f]\n",
        "          \n",
        "          \n",
        "          ls[d] = tuple(np.floor(prebin).astype(np.int)) ## MAYBE JUST RETURN l's and then do <unique,count>\n",
        "          #if not l in cmsketch:\n",
        "          #    cmsketch[l] = 0\n",
        "          #cmsketch[l] += 1\n",
        "\n",
        "        #self.cmsketches[depth] = cmsketch\n",
        "\n",
        "        return ls #<-- contains the integer bind-id vector at each level for the *single* X\n",
        "\n",
        "\n",
        "    def bincount_score(self, X, cmsketches):\n",
        "        # calculate the score at every depth\n",
        "\n",
        "        scores = np.zeros(self.depth)\n",
        "       \n",
        "        prebin = np.zeros(len(X), dtype=np.float)\n",
        "        depthcount = np.zeros(len(self.deltamax), dtype=np.int)\n",
        "  \n",
        "        for d in range(self.depth):\n",
        "            f = self.fs[d] \n",
        "            depthcount[f] += 1\n",
        "            if depthcount[f] == 1:\n",
        "                prebin[f] = (X[f] + self.shift[f])/self.deltamax[f]\n",
        "            else:\n",
        "                prebin[f] = 2.0*prebin[f] - self.shift[f]/self.deltamax[f]\n",
        "\n",
        "            cmsketch = cmsketches[d]\n",
        "            l = tuple(np.floor(prebin).astype(np.int))\n",
        "\n",
        "            rowcols = cms.findAllRowCols(l)\n",
        "            counts = [cmsketch.get(rowcol, 0) for rowcol in rowcols]\n",
        "            #print(d, counts)\n",
        "            scores[d] = min(counts)\n",
        "            \n",
        "        \n",
        "        # scale score logarithmically to avoid overflow:\n",
        "        # score = min_d [ log2(bincount x 2^d) = log2(bincount) + d ]\n",
        "        depths = np.array([d for d in range(1, self.depth+1)])\n",
        "        scores = np.log2(1.0 + scores) + depths \n",
        "        return np.min(scores)\n",
        "\n",
        "    def bincount_score_exp(self, X, cmsketches):\n",
        "        # calculate the score at every depth\n",
        "\n",
        "        scoresAllDepths = np.zeros(self.depth)\n",
        "        \n",
        "        prebin = np.zeros(len(X), dtype=np.float)\n",
        "        depthcount = np.zeros(len(self.deltamax), dtype=np.int)\n",
        "  \n",
        "        for d in range(self.depth):\n",
        "            f = self.fs[d] \n",
        "            depthcount[f] += 1\n",
        "            if depthcount[f] == 1:\n",
        "                prebin[f] = (X[f] + self.shift[f])/self.deltamax[f]\n",
        "            else:\n",
        "                prebin[f] = 2.0*prebin[f] - self.shift[f]/self.deltamax[f]\n",
        "\n",
        "            cmsketch = cmsketches[d]\n",
        "            l = tuple(np.floor(prebin).astype(np.int))\n",
        "\n",
        "            rowcols = cms.findAllRowCols(l)\n",
        "            counts = [cmsketch.get(rowcol, 0) for rowcol in rowcols]\n",
        "            #print(d, counts)\n",
        "            scoresAllDepths[d] = min(counts)\n",
        "            \n",
        "        #print(scores, scores_2)\n",
        "        \n",
        "        # scale score logarithmically to avoid overflow:\n",
        "        # score = min_d [ log2(bincount x 2^d) = log2(bincount) + d ]\n",
        "        depths = np.array([d for d in range(1, self.depth+1)])\n",
        "        scoresAllDepths = np.log2(1.0 + scoresAllDepths) + depths \n",
        "        return scoresAllDepths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FtXLnEH1TMl"
      },
      "outputs": [],
      "source": [
        "from multiprocessing.pool import ThreadPool\n",
        "from functools import reduce\n",
        "from operator import add\n",
        "\n",
        "class Chains:\n",
        "    def __init__(self, deltamax, nchains=1, depth=2, nthreads_fit=1, nthreads_score=128, samplerate=0.1): #k=3 , seed=42\n",
        "        self.nchains = nchains\n",
        "        self.depth = depth\n",
        "        self.nthreads_fit = nthreads_fit\n",
        "        self.nthreads_score = nthreads_score\n",
        "        self.samplerate = samplerate\n",
        "        self.deltamax = deltamax\n",
        "        self.chains_cmsketches = [None] * nchains # will be made a boardcast variable after fitting\n",
        "        self.chains = [None] * nchains\n",
        "        #self.projector = StreamhashProjection(n_components=k, density=1/3.0, random_state=seed)  # StreamhashProjection(n_components=k, density=1/3.0, random_state=seed)\n",
        "\n",
        "    def fitparallel(self, allX):   \n",
        "        pool = ThreadPool(self.nthreads_fit)\n",
        "\n",
        "        def fitone(cindex):     \n",
        "\n",
        "            # create i'th Chain\n",
        "            c = Chain(depth=self.depth, deltamax=self.deltamax)\n",
        "            self.chains[cindex] = c\n",
        "\n",
        "            binvecsRDD = allX.sample(False,self.samplerate, cindex*100).map(lambda x: c.fit(x[0])) # <-- inject random sampling\n",
        "\n",
        "            cmsketches = [{}] * self.depth\n",
        "            for d in range(self.depth):\n",
        "                cmsketches[d] = binvecsRDD.flatMap(lambda x: cms.allCols(x[d])).reduceByKey(lambda x,y:x+y).collectAsMap()\n",
        "            self.chains_cmsketches[cindex] = cmsketches\n",
        "            \n",
        "            return [cindex]\n",
        "        \n",
        "        parameters = list(range(self.nchains))\n",
        "        pool.map(lambda chainindex: fitone(chainindex), parameters)\n",
        "\n",
        "        return self\n",
        "    \n",
        "\n",
        "    def fit(self, allX):     \n",
        "        for i in tqdm.tqdm(range(self.nchains), desc='Fitting...'): # <-- THREADS?\n",
        "            # create i'th Chain\n",
        "            c = Chain(depth=self.depth, deltamax=self.deltamax)\n",
        "            self.chains[cindex] = c\n",
        "\n",
        "            #binvecsRDD = allX.rdd.map(lambda x: c.fit(x)) # <-- inject random sampling\n",
        "            binvecsRDD = allX.rdd.sample(False,self.samplerate,i*100).map(lambda x: c.fit(x)) # <-- inject random sampling\n",
        "            #binvecsRDD = allX.rdd.filter(inside).map(lambda x: c.fit(x)) # <-- inject random sampling\n",
        "\n",
        "            cmsketches = [{}] * self.depth\n",
        "            for d in range(self.depth):\n",
        "                cmsketches[d] = binvecsRDD.map(lambda x: (x[d], 1)).reduceByKey(lambda x,y:x+y).collectAsMap()\n",
        "            self.chains_cmsketches[cindex] = cmsketches\n",
        "        \n",
        "        return self\n",
        "\n",
        "    def scoreparallel_test(self, allX):\n",
        "        time_a = time.time()\n",
        "        pool = ThreadPool(self.nthreads_score)\n",
        "\n",
        "        def scoreone(cindex):     \n",
        "            chain = self.chains[cindex]\n",
        "            scores_rdd = allX.map(lambda x:(chain.bincount_score(x[0], self.chains_cmsketches[cindex]), x[1]))\n",
        "            return scores_rdd.zipWithIndex().map(lambda x:(x[1], x[0])) # (index, (score, label))\n",
        "        \n",
        "        parameters = list(range(self.nchains))\n",
        "        all_score_rdds = pool.map(lambda chainindex: scoreone(chainindex), parameters)\n",
        "        assert(len(all_score_rdds) == self.nchains)\n",
        "        time_b = time.time()\n",
        "        print(\"Score time = \", time_b-time_a)\n",
        "        \n",
        "        sum_rdd = all_score_rdds[0]\n",
        "        for i in range(1, self.nchains):\n",
        "            score_rdd = all_score_rdds[i]\n",
        "            sum_rdd = sum_rdd + score_rdd # concatenate\n",
        "            \n",
        "        totalscore_rdd = sum_rdd.reduceByKey(lambda x,y:(x[0]+y[0], x[1])) # key is index, value is (totalscore,label)\n",
        "        time_c = time.time()\n",
        "        print(\"Combine time new = \", time_c-time_b)\n",
        "\n",
        "        return totalscore_rdd.map(lambda x: ( x[0], (float(-x[1][0]/self.nchains), float(x[1][1])) ) ) #  key is index, value is (score,label)\n",
        " \n",
        "\n",
        "    def score(self, X):\n",
        "        #scores = np.zeros(allX.count())\n",
        "        #chain = self.chains[0]\n",
        "        totalscore = 0;\n",
        "        #scoresRDD = allX.rdd.map(lambda x: chain.bincount_score(x, self.chains_cmsketches[0]))\n",
        "        for i in range(self.nchains): # <-- THREADS?\n",
        "            chain = self.chains[i]\n",
        "            totalscore += chain.bincount_score(X, self.chains_cmsketches[i])\n",
        "        return -totalscore/float(self.nchains)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_28G9jnaUr_"
      },
      "source": [
        "## Exec: Min-Max Normalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aOQv5zeZwlx",
        "outputId": "f16bf416-6e7b-45fb-d032-d5ccba285762"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Min-max normalize\n",
            "['_c0', '_c1', '_c2', '_c3']\n",
            "_c2\n",
            "_c3\n",
            "+------+--------+------------------+-------------------+---+\n",
            "|   _c0|     _c1|               _c2|                _c3|_c4|\n",
            "+------+--------+------------------+-------------------+---+\n",
            "|Female|Bulgaria|               0.0|                1.0|  1|\n",
            "|Female|  Turkey|0.6571428571428571|                0.0|  1|\n",
            "|  Male|Bulgaria|0.6857142857142857|0.08163265306122448|  0|\n",
            "|  Male|Bulgaria|0.6857142857142857|0.08163265306122448|  0|\n",
            "|  Male|   India|0.6285714285714286|0.10204081632653061|  0|\n",
            "|  Male|Bulgaria|0.7428571428571429|0.12244897959183673|  0|\n",
            "|  Male| Germany|               1.0|0.05612244897959184|  0|\n",
            "+------+--------+------------------+-------------------+---+\n",
            "\n",
            "Time = 4.198s\n"
          ]
        }
      ],
      "source": [
        "    all_outputs = []\n",
        "    min_max = True\n",
        "    if min_max:\n",
        "        print(\"Min-max normalize\")\n",
        "        all_outputs.append(\"Min-max normalize\")\n",
        "        start = time.time()\n",
        "        normDF = min_max_normalize(myDF) #100+ columns\n",
        "        normDF.show()\n",
        "        end = time.time()\n",
        "        print(\"Time = %.3fs\" % (end-start))\n",
        "        all_outputs.append(\"Time = %.3fs\" % (end-start))\n",
        "    else:\n",
        "        normDF = myDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rg2SooHIabkb"
      },
      "source": [
        "## Exec: Project "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHHReQ7kZ_V_",
        "outputId": "2fe9dd2f-b3cb-45d9-99b4-fa864977a95b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Projection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  del sys.path[0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.31622777 0.31622777 0.31622777 0.31622777 0.16537421 0.31622777\n",
            " 0.31622777 0.15811388 0.         0.15811388 0.15811388 0.31622777\n",
            " 0.31622777 0.30735403 0.31622777 0.         0.15359634 0.15811388\n",
            " 0.30332051 0.15811388 0.37931198 0.30735403 0.31622777 0.\n",
            " 0.15811388 0.15811388 0.15811388 0.15811388 0.30735403 0.        ]\n",
            "Time = 87.585s\n"
          ]
        }
      ],
      "source": [
        "    print(\"Projection\")\n",
        "    all_outputs.append(\"Projection\")\n",
        "    start = time.time()\n",
        "    if projection:\n",
        "        pDF = projectDF(normDF, projdim, isMutable, sc=sc)\n",
        "    else:\n",
        "        pDF = normDF\n",
        "    deltamax = feature_range(pDF)\n",
        "    print(deltamax)\n",
        "    # convert into pair rdd\n",
        "    pDF_pair = pDF.rdd.map(lambda x: (list(x[0:-1]), x[-1]))\n",
        "    pDF_pair.cache()\n",
        "    end = time.time()\n",
        "    print(\"Time = %.3fs\" % (end-start))\n",
        "    all_outputs.append(\"Time = %.3fs\" % (end-start))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ie0XeLd-sNLz"
      },
      "source": [
        "## Exec: Fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1HbWZw3sPQt",
        "outputId": "770a64f7-13c6-4dd0-93c3-ccefbefc2af5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting\n",
            "[28, 14, 10, 7, 28, 20, 6, 25, 18, 22, 10, 10, 23, 20, 3]\n",
            "[27, 14, 27, 6, 11, 28, 7, 14, 2, 13, 16, 3, 17, 7, 3][1, 29, 5, 21, 9, 3, 21, 28, 17, 25, 11, 1, 9, 29, 3]\n",
            "[22, 29, 23, 4, 2, 11, 7, 21, 26, 2, 0, 2, 4, 14, 13][2, 0, 4, 25, 22, 13, 6, 26, 8, 14, 14, 25, 9, 27, 12]\n",
            "\n",
            "[5, 23, 4, 19, 1, 5, 21, 10, 15, 15, 0, 8, 27, 26, 5]\n",
            "[19, 23, 21, 23, 0, 23, 19, 10, 16, 7, 3, 5, 7, 19, 29]\n",
            "[2, 15, 29, 24, 2, 24, 28, 17, 13, 17, 1, 21, 2, 15, 28]\n",
            "\n",
            "[27, 25, 6, 24, 3, 12, 19, 0, 7, 13, 15, 13, 11, 22, 25]\n",
            "[4, 25, 20, 29, 22, 8, 11, 20, 0, 25, 25, 0, 14, 1, 29]\n",
            "[24, 7, 12, 20, 0, 15, 28, 6, 4, 21, 28, 22, 2, 11, 25]\n",
            "[10, 4, 11, 2, 0, 0, 7, 9, 10, 11, 28, 12, 11, 13, 1]\n",
            "[16, 29, 19, 15, 24, 21, 12, 26, 18, 16, 3, 11, 28, 8, 18]\n",
            "[17, 2, 27, 0, 0, 28, 18, 12, 3, 3, 5, 27, 18, 28, 11]\n",
            "[15, 7, 5, 11, 23, 27, 20, 7, 25, 27, 3, 25, 7, 25, 27]\n",
            "[12, 25, 21, 9, 17, 18, 4, 14, 24, 1, 9, 23, 29, 17, 26]\n",
            "[24, 27, 6, 25, 25, 1, 21, 21, 2, 26, 17, 11, 0, 21, 11]\n",
            "[14, 24, 22, 22, 15, 10, 21, 15, 7, 3, 7, 3, 5, 2, 18]\n",
            "[17, 7, 22, 2, 18, 7, 4, 3, 23, 5, 8, 1, 11, 2, 2]\n",
            "[14, 14, 28, 0, 19, 29, 6, 26, 3, 28, 5, 24, 0, 28, 14]\n",
            "[13, 14, 0, 20, 21, 21, 10, 14, 27, 12, 19, 23, 14, 28, 7]\n",
            "[4, 22, 23, 23, 3, 28, 13, 7, 8, 21, 10, 22, 0, 13, 4]\n",
            "[22, 27, 17, 20, 16, 6, 16, 19, 8, 16, 19, 21, 21, 27, 28]\n",
            "[7, 0, 24, 22, 2, 23, 22, 26, 28, 7, 21, 24, 14, 4, 2]\n",
            "[18, 0, 27, 14, 7, 1, 2, 23, 13, 23, 13, 10, 15, 3, 19]\n",
            "[24, 29, 17, 16, 17, 18, 16, 13, 8, 7, 1, 12, 24, 6, 8]\n",
            "[15, 4, 28, 4, 5, 18, 7, 15, 12, 18, 20, 3, 18, 25, 28]\n",
            "[6, 29, 23, 22, 28, 16, 9, 24, 22, 16, 14, 11, 29, 15, 23]\n",
            "[16, 10, 9, 14, 9, 15, 6, 16, 22, 25, 20, 21, 25, 6, 13][18, 7, 9, 20, 6, 8, 18, 15, 7, 17, 26, 22, 28, 17, 29]\n",
            "\n",
            "[4, 29, 27, 11, 0, 2, 17, 26, 9, 6, 16, 13, 23, 2, 11]\n",
            "[0, 2, 28, 12, 13, 22, 24, 6, 19, 4, 0, 29, 16, 2, 12]\n",
            "[8, 10, 14, 23, 2, 23, 5, 4, 4, 21, 14, 22, 16, 8, 2]\n",
            "[15, 28, 5, 19, 19, 5, 9, 2, 17, 17, 29, 16, 23, 2, 2]\n",
            "[21, 18, 17, 25, 5, 3, 12, 23, 5, 12, 23, 28, 14, 24, 8]\n",
            "[2, 7, 0, 5, 23, 18, 15, 7, 27, 0, 26, 26, 15, 8, 12]\n",
            "[2, 8, 23, 4, 26, 9, 0, 21, 12, 24, 20, 9, 3, 26, 5]\n",
            "[20, 4, 17, 27, 27, 10, 3, 12, 25, 19, 6, 8, 16, 11, 24]\n",
            "[12, 29, 10, 10, 20, 19, 29, 14, 26, 6, 10, 19, 15, 3, 5]\n",
            "[5, 25, 21, 25, 2, 8, 27, 9, 13, 10, 17, 21, 11, 22, 11]\n",
            "[29, 16, 29, 3, 21, 1, 26, 24, 24, 27, 1, 2, 4, 4, 0][10, 9, 14, 26, 18, 14, 19, 25, 16, 28, 26, 10, 7, 11, 23]\n",
            "\n",
            "[27, 4, 8, 3, 26, 21, 16, 8, 0, 20, 19, 12, 27, 15, 3]\n",
            "[19, 12, 2, 26, 28, 17, 28, 25, 18, 8, 5, 4, 20, 26, 4]\n",
            "[22, 2, 15, 18, 1, 17, 16, 25, 7, 14, 7, 16, 11, 26, 7]\n",
            "[12, 16, 22, 4, 21, 15, 5, 7, 24, 17, 24, 11, 14, 26, 15]\n",
            "[1, 11, 18, 4, 11, 26, 26, 16, 14, 16, 9, 5, 17, 12, 2]\n",
            "[13, 20, 20, 19, 22, 20, 25, 29, 3, 1, 19, 15, 27, 17, 7]\n",
            "[29, 10, 29, 25, 27, 9, 20, 3, 11, 26, 7, 1, 21, 12, 20]\n",
            "[24, 10, 11, 13, 4, 21, 21, 21, 10, 24, 21, 27, 28, 12, 17]\n",
            "Time = 1556.913s\n"
          ]
        }
      ],
      "source": [
        "    print(\"Fitting\")\n",
        "    all_outputs.append(\"Fitting\")\n",
        "    start = time.time()\n",
        "    cf = Chains(deltamax, nchains=nchains, depth=depth, nthreads_fit=nthreads_fit, nthreads_score=nthreads_score, samplerate=samplerate)\n",
        "    cf = cf.fitparallel(pDF_pair)\n",
        "    # saving cf\n",
        "    # pickle.dump(cf, open(model_file, \"wb\"))\n",
        "    end = time.time()\n",
        "    print(\"Time = %.3fs\" % (end-start))\n",
        "    all_outputs.append(\"Time = %.3fs\" % (end-start))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a77vItl3vdzI"
      },
      "source": [
        "## Exec: Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLnq7oiavgqF",
        "outputId": "d4617fe4-5eda-45d5-b733-e5c39109ed1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scoring\n",
            "Score time =  64.4252827167511\n",
            "Combine time new =  6.289279937744141\n"
          ]
        }
      ],
      "source": [
        "    print(\"Scoring\")\n",
        "    all_outputs.append(\"Scoring\")\n",
        "    \n",
        "    anomalyScoresRDD = cf.scoreparallel_test(pDF_pair) #(index, (score, label))\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9c2dQMuvxYw"
      },
      "source": [
        "## Exec: Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgC4n4U6vzZo",
        "outputId": "fe2b180d-7b24-4e2e-da93-97a8b4ac179a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time = 1556.913s\n",
            "xstream: AUC = 1.0\n",
            "xstream: AP = 1.0\n"
          ]
        }
      ],
      "source": [
        "    scoreslabelsRDD = anomalyScoresRDD.values()\n",
        "    \n",
        "    metrics = BinaryClassificationMetrics(scoreslabelsRDD)\n",
        "    auc = metrics.areaUnderROC\n",
        "    ap = metrics.areaUnderPR\n",
        "    \n",
        "    #end = time.time()\n",
        "    print(\"Time = %.3fs\" % (end-start))\n",
        "    #all_outputs.append(\"Time = %.3fs\" % (end-start))\n",
        "    \n",
        "    print(\"xstream: AUC =\", auc)\n",
        "    #all_outputs.append(\"xstream: AUC = %.5f\" % auc)\n",
        "    \n",
        "    print(\"xstream: AP =\", ap)\n",
        "    #all_outputs.append(\"xstream: AP = %.5f\" % ap)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sB_ALmhHaEUA"
      },
      "source": [
        "## Exec: Explain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfxVp0dGn3ep",
        "outputId": "4c8da452-6aca-4bc9-c3a1-37742aaf250a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, (-2.684620955777741, 1.0)), (1, (-2.836212222467418, 1.0)), (6, (-3.004922588511942, 0.0))]\n"
          ]
        }
      ],
      "source": [
        "pDFpairKVrdd = pDF_pair.zipWithIndex().map(lambda x:(x[1], x[0])) # (index, (features, label))\n",
        "\n",
        "topk=3\n",
        "top = anomalyScoresRDD.takeOrdered(topk, key=lambda x:-x[1][0])\n",
        "print(top)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I08veFUEaGsr",
        "outputId": "d5676f35-33ae-451f-8c9d-6dff18629ded"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, ([0.3162277660168379, 0.6324555320336758, 0.6324555320336758, 0.0, 0.3162277660168379, 0.6324555320336758, 0.9486832980505137, 0.0, 0.0, 0.0, 0.3162277660168379, 0.6324555320336758, 0.3162277660168379, 0.6324555320336758, 0.0, 0.0, 0.3162277660168379, 0.3162277660168379, 0.6324555320336758, 0.0, 0.3162277660168379, 0.6324555320336758, 0.0, 0.0, 0.3162277660168379, 0.3162277660168379, 0.6324555320336758, 0.3162277660168379, 0.6324555320336758, 0.0], 1))]\n",
            "['Female', 'Bulgaria', 0.0, 1.0, 1]\n"
          ]
        }
      ],
      "source": [
        "xpind = 0 # top anomaly is at index 0\n",
        "\n",
        "topanomaly = pDFpairKVrdd.filter(lambda x: x[0] == top[xpind][0]).values()\n",
        "print(pDFpairKVrdd.filter(lambda x: x[0] == top[xpind][0]).collect())\n",
        "\n",
        "normDFKVrdd = normDF.rdd.zipWithIndex().map(lambda x:(x[1], x[0])) # (index, rawrow)\n",
        "topORIGanomaly = normDFKVrdd.filter(lambda x: x[0] == top[xpind][0]).values().collect()  \n",
        "print(list(topORIGanomaly[0])) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uW_VQlJsjkW",
        "outputId": "58d9ea9d-2090-4948-9d17-ce6e3bee511c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-2.684620955777741\n"
          ]
        }
      ],
      "source": [
        "# find scores by in- and out- chains per (projection or original) feature\n",
        "features = pDF.columns[:-1]\n",
        "\n",
        "#score from chains that do and do NOT factor f in their anomaly score\n",
        "fused = np.zeros((len(features), cf.nchains))\n",
        "\n",
        "score_in_c = np.zeros(len(features))\n",
        "score_out_c = np.zeros(len(features))\n",
        "\n",
        "anomaly_score = 0\n",
        "for cindex in range(0, cf.nchains):\n",
        "  c = cf.chains[cindex]\n",
        "\n",
        "  score_c_AllDepths = topanomaly.map(lambda x: c.bincount_score_exp(x[0], cf.chains_cmsketches[cindex]) ).collect()\n",
        "  #print(score_c_AllDepths)\n",
        "\n",
        "  ind_min = np.argmin(score_c_AllDepths)\n",
        "  #print(ind_min)\n",
        "  if not np.isscalar(ind_min):\n",
        "    ind_min = ind_min[0]\n",
        "\n",
        "  score_c = score_c_AllDepths[0][ind_min]\n",
        "  anomaly_score = anomaly_score + score_c\n",
        "  \n",
        "  cSplitFeatures = c.fs\n",
        "  \n",
        "  #print(cSplitFeatures[0:ind_min+1])\n",
        "\n",
        "  for f in range(len(features)):     \n",
        "    if f in cSplitFeatures[0:ind_min+1]:\n",
        "      fused[f,cindex] = 1\n",
        "      score_in_c[f] = score_in_c[f] + score_c\n",
        "    else:\n",
        "      score_out_c[f] = score_out_c[f] + score_c\n",
        "\n",
        "anomaly_score = float(-anomaly_score/cf.nchains)\n",
        "print(anomaly_score)  \n",
        "# print(score_in_c)\n",
        "# print(score_out_c)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyGzUgyA3VrV",
        "outputId": "dd76dc8f-c61e-45b3-fbd3-71f0dd3b2a6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.0\n",
            "2.0\n",
            "4.0\n",
            "0.0\n",
            "4.0\n",
            "2.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "2.0\n",
            "0.0\n",
            "3.0\n",
            "2.0\n",
            "3.0\n",
            "3.0\n",
            "3.0\n",
            "2.0\n",
            "2.0\n",
            "2.0\n",
            "1.0\n",
            "1.0\n",
            "3.0\n",
            "0.0\n",
            "4.0\n",
            "1.0\n",
            "0.0\n",
            "3.0\n",
            "2.0\n",
            "2.0\n",
            "Importance of (projection) features BY SCORE DIFFERENCE: \n",
            "[-0.39576671  0.71314683  0.74415321  0.         -0.34280331  0.71314683\n",
            "  0.69859281 -0.91871586 -1.34222351 -0.32181535 -0.63318781  0.\n",
            "  0.72832017  0.71314683 -0.04115679 -1.04472948 -0.39576671  0.71314683\n",
            " -0.49619072  0.71314683  0.1016923   0.69859281  0.72832017  0.\n",
            " -1.22036301 -0.32181535  0.         -0.33550962  0.1923135  -1.15402114]\n",
            "Importance of (projection) features BY REDUCTION IN SCORE\n",
            "[-0.023746    0.02852587  0.05953226  0.         -0.02742426  0.02852587\n",
            "  0.01397186 -0.01837432 -0.02684447 -0.00643631 -0.02532751  0.\n",
            "  0.04369921  0.02852587 -0.00246941 -0.06268377 -0.023746    0.02852587\n",
            " -0.01984763  0.02852587  0.00203385  0.01397186  0.04369921  0.\n",
            " -0.09762904 -0.00643631  0.         -0.02013058  0.00769254 -0.04616085]\n",
            "[-0.41951272  0.7416727   0.80368547  0.         -0.37022757  0.7416727\n",
            "  0.71256467 -0.93709018 -1.36906798 -0.32825166 -0.65851532  0.\n",
            "  0.77201938  0.7416727  -0.04362619 -1.10741325 -0.41951272  0.7416727\n",
            " -0.51603835  0.7416727   0.10372615  0.71256467  0.77201938  0.\n",
            " -1.31799205 -0.32825166  0.         -0.3556402   0.20000604 -1.20018199]\n"
          ]
        }
      ],
      "source": [
        "# find importance of each (projection or original) feature\n",
        "\n",
        "## 1. BY SCORE DIFFERENCE\n",
        "fimportance = np.zeros(len(features))\n",
        "for f in range(len(features)):\n",
        "  num_in = sum(fused[f,:])\n",
        "  print(num_in.T)\n",
        "  if num_in != 0:\n",
        "    if (cf.nchains - num_in) != 0:\n",
        "      fimportance[f] = float(-score_in_c[f]/num_in) - float(- score_out_c[f]/(cf.nchains - num_in))\n",
        "\n",
        "print(\"Importance of (projection) features BY SCORE DIFFERENCE: \")\n",
        "# the larger and positive, the better\n",
        "print(fimportance)\n",
        "\n",
        "## 2. BY \"REDUCTION\" (may not always be positive?) IN SCORE\n",
        "fimportance2 = np.zeros(len(features))\n",
        "for f in range(len(features)):\n",
        "  num_in = sum(fused[f,:])\n",
        "\n",
        "  if num_in != 0:\n",
        "    if (cf.nchains - num_in) != 0:\n",
        "      fimportance2[f] =  anomaly_score - float(- score_out_c[f]/(cf.nchains - num_in))\n",
        "# the larger and positive, the better\n",
        "print(\"Importance of (projection) features BY REDUCTION IN SCORE\")\n",
        "print(fimportance2)\n",
        "\n",
        "## 3. addition of those two\n",
        "fimportance3 = fimportance2 + fimportance\n",
        "#fimportance3 = fimportance3 / fimportance3.sum() !!!! IF THE SUM IS NEGATIVE, TROUBLE\n",
        "print(fimportance3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Rftl2I7rjo6"
      },
      "outputs": [],
      "source": [
        "### R_ohe IS THE SAME FOR ALL EXPLANATIONS: BUILD ONCE ONLY\n",
        "if projection: ## ----------------> moved to explain_transform_ohe\n",
        "    feature_names = all_columns\n",
        "    ndim = len(feature_names)\n",
        "\n",
        "    X = normDF.take(1)\n",
        "    #print(X)\n",
        "    types = [type(X[0][i]) == str for i in range(ndim)]\n",
        "    #print(types)\n",
        "    # find total # of features after OHE\n",
        "    num_unique = np.zeros((ndim,1))\n",
        "    for i in range(ndim):\n",
        "        if types[i]:\n",
        "          num_unique[i] = normDF.select(normDF.columns[i]).distinct().count()\n",
        "        else:\n",
        "          num_unique[i] = 1\n",
        "\n",
        "    #print(num_unique)\n",
        "\n",
        "    totalAfterOHE = num_unique.sum()\n",
        "    \n",
        "    R_ohe = np.zeros((projdim, int(totalAfterOHE))) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sS30EWPNmjGO",
        "outputId": "6b8b2359-f6f1-4f33-8a99-1ac167ba632f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Female\n",
            "Male\n",
            "Turkey\n",
            "Germany\n",
            "India\n",
            "Bulgaria\n",
            "[[1. 0. 1. 0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 1. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 1. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 1. 0.]\n",
            " [0. 0. 1. 0. 1. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [1. 1. 0. 0. 1. 1. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "### R_ohe IS THE SAME FOR ALL EXPLANATIONS: BUILD ONCE ONLY\n",
        "\n",
        "#i=1  ## ----------------> moved to explain_transform_ohe\n",
        "# distinctDF = normDF.select(normDF.columns[i]).distinct()\n",
        "#dvals = distinctDF.rdd.collect()\n",
        "#print(dvals[0][0])\n",
        "#outrdd = distinctDF.rdd.map(lambda x: (x[0], np.array([_hash_string(k, x[0]) for k in keys])))\n",
        "#print(outrdd.collect())\n",
        "if projection:\n",
        "  def _hash_string(k, s):\n",
        "      hash_value = int(mmh3.hash(s, signed=False, seed=k))/(2.0**32-1)\n",
        "      den = 1/3\n",
        "      if hash_value <= den/2.0:\n",
        "          return 1 #-1 \n",
        "      elif hash_value <= den:\n",
        "          return 1\n",
        "      else:\n",
        "          return 0\n",
        "\n",
        "\n",
        "  random.seed(42)\n",
        "  keys = np.arange(0, projdim, 1)\n",
        "  ind = 0\n",
        "  for i in range(ndim): # for each original feature\n",
        "      if types[i]:\n",
        "          # find all distinct values of an original categorical feature:\n",
        "          distinctDF = normDF.select(normDF.columns[i]).distinct()\n",
        "          # hash each unique name.value combination K times\n",
        "          outrdd = distinctDF.rdd.map(lambda x: (x[0], np.array([_hash_string(k, feature_names[i]+'.'+x[0]) for k in keys])))\n",
        "          for v in outrdd.collect():\n",
        "              print(v[0])\n",
        "              #print(abs(v[1]))\n",
        "              R_ohe[:,ind] = v[1].T #abs(v[1]).T\n",
        "              #print(abs(v[1]).sum())\n",
        "              ind = ind+1\n",
        "      else:\n",
        "        f = feature_names[i]\n",
        "        R_ohe[:,ind] = np.array([_hash_string(k, f) for k in keys]) #abs(np.array([_hash_string(k, f) for k in keys]))\n",
        "        ind = ind+1 # increment by 1 as just 1 ohe feature\n",
        "\n",
        "  np.set_printoptions(threshold=np.inf)\n",
        "  np.set_printoptions(linewidth=200)\n",
        "  print(R_ohe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWpZv1WXXbIT"
      },
      "outputs": [],
      "source": [
        "#if projection:\n",
        "  \n",
        "  #feature_names = all_columns\n",
        "  #print(feature_names)\n",
        "\n",
        "  # projector = StreamhashProjection(n_components=projdim, \n",
        "  #                                   feature_names=feature_names,\n",
        "  #                                   density=1/3.0, \n",
        "  #                                   random_state=42, \n",
        "  #                                   mutable=True,\n",
        "  #                                   sc=sc)\n",
        "\n",
        "  # hashR = projector.explain_transform_ohe(normDF, feature_names)\n",
        "\n",
        "  # #### newR is a KXd binary matrix, rij=1 means projection i contains feature j\n",
        "  # newR = np.abs(hashR) / projector.constant\n",
        "  # print(newR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ulm2sAZNW7w7",
        "outputId": "cf9bae40-a08f-49a1-9514-d1f1359276e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5. 2. 3. 1. 5. 4. 1. 2.]\n",
            "[3. 3. 3. 2. 3. 2. 5. 1. 0. 1.]\n",
            "[-0.42140267  0.81855447  0.72272923  0.82435275 -0.4681285  -0.30333482  0.         -0.95090915 -1.12106027 -0.21242877]\n",
            "[[-0.42140267 -0.         -0.42140267 -0.         -0.42140267 -0.         -0.         -0.        ]\n",
            " [ 0.81855447  0.          0.          0.          0.81855447  0.81855447  0.          0.        ]\n",
            " [ 0.72272923  0.          0.          0.          0.72272923  0.72272923  0.          0.        ]\n",
            " [ 0.          0.          0.          0.82435275  0.          0.          0.82435275  0.        ]\n",
            " [-0.         -0.         -0.4681285  -0.         -0.4681285  -0.         -0.         -0.4681285 ]\n",
            " [-0.30333482 -0.         -0.         -0.         -0.         -0.30333482 -0.         -0.        ]\n",
            " [ 0.          0.          0.          0.          0.          0.          0.          0.        ]\n",
            " [-0.         -0.         -0.95090915 -0.         -0.         -0.         -0.         -0.        ]\n",
            " [-0.         -0.         -0.         -0.         -0.         -0.         -0.         -0.        ]\n",
            " [-0.         -0.21242877 -0.         -0.         -0.         -0.         -0.         -0.        ]]\n",
            "[ 0.16330924 -0.10621439 -0.61348011  0.82435275  0.13035051  0.30948722  0.82435275 -0.23406425]\n"
          ]
        }
      ],
      "source": [
        "# attribution to original features\n",
        "# 1. (Naive) BY AVG PROJECTION-FEATURE IMPORTANCES\n",
        "if projection: \n",
        "  fimpr = fimportance3.copy() # which one to use\n",
        "\n",
        "  totalprojperfeat = np.sum(R_ohe, axis=0)\n",
        "  print(totalprojperfeat)\n",
        "  totalfeatureperproj = np.sum(R_ohe, axis=1)\n",
        "  print(totalfeatureperproj)\n",
        "\n",
        "  print(fimpr)\n",
        "  scaledR = R_ohe.copy()\n",
        "  for p in range(len(features)):\n",
        "      scaledR[p,:] = scaledR[p,:] * fimpr[p]\n",
        "\n",
        "  np.set_printoptions(threshold=np.inf)\n",
        "  np.set_printoptions(linewidth=200)\n",
        "  print(scaledR)\n",
        "  #print(scaledR.T)\n",
        "  origfimportance = np.sum(scaledR, axis=0) / totalprojperfeat\n",
        "\n",
        "  print(origfimportance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLny6AJgXDUM",
        "outputId": "1cee6079-1d7c-44b4-c544-cdb10ab0c992"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of original features  8\n",
            "Number of projected features  10\n",
            "[ 0.69958179 -0.72608848 -0.59186913 -0.29578369 -0.10775036 -0.29170927  0.34287704  0.91669548 -1.13578369  0.06929278]\n",
            "-1.1205375288621222\n",
            "[[1 2 3 4 5 8]]\n",
            "[[1. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
            "[0.69958179 0.         0.         0.         0.         0.         0.34287704 0.91669548 0.         0.06929278]\n",
            "2.0284470917293373\n",
            "1.0\n",
            "1.0\n",
            "[[0.33333333 0.         0.         0.         0.         0.         0.2        0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.2        0.         0.         1.        ]\n",
            " [0.33333333 0.         0.         0.         0.         0.         0.         1.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.33333333 0.         0.         0.         0.         0.         0.2        0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.2        0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.2        0.         0.         0.        ]]\n",
            "[[0.14979419]\n",
            " [0.06834242]\n",
            " [0.56370585]\n",
            " [0.        ]\n",
            " [0.14979419]\n",
            " [0.03418168]\n",
            " [0.        ]\n",
            " [0.03418168]]\n"
          ]
        }
      ],
      "source": [
        "# attribution to original features\n",
        "# 1. BY RANDOM WALKS\n",
        "\n",
        "dr = np.random.rand(R_ohe.shape[1],1)\n",
        "pr = np.random.rand(R_ohe.shape[0],1)\n",
        "\n",
        "\n",
        "sumr = dr.sum()+pr.sum()\n",
        "dr = dr / sumr\n",
        "pr = pr / sumr\n",
        "print(\"Number of original features \", R_ohe.shape[1])\n",
        "print(\"Number of projected features \", R_ohe.shape[0])\n",
        "alpha = 0.15\n",
        "\n",
        "fimpr = fimportance3.copy() # which one to use\n",
        "print(fimpr)\n",
        "print(fimpr.sum())\n",
        "\n",
        "newR = R_ohe.copy()\n",
        "\n",
        "#Option 1: # Ignore negative (projection) features\n",
        "indices = np.array(np.where(fimpr < 0))\n",
        "          #Remove associated edges\n",
        "print(indices)\n",
        "newR[indices[0],:] =0 \n",
        "print(newR.T)\n",
        "\n",
        "fimpr[fimpr < 0] = 0 \n",
        "\n",
        "#Option 2: # Shift so that minimum is zero\n",
        "#if np.min(fimpr) < 0:\n",
        "#  fimpr = fimpr + np.abs(np.min(fimpr))  \n",
        "\n",
        "print(fimpr) \n",
        "print(fimpr.sum())\n",
        "fimpr = fimpr / sum(fimpr) # normalize so sum to 1, a prob dist.n\n",
        "\n",
        "fimpr = fimpr.reshape(newR.shape[0],1)\n",
        "#print(fimpr)\n",
        "print(fimpr.sum())\n",
        "print(dr.sum()+pr.sum())\n",
        "\n",
        "sums = newR.sum(axis=0,keepdims=1)\n",
        "#print(sums)\n",
        "sums[sums==0] = 1\n",
        "A = newR.copy() / sums\n",
        "#print(A)\n",
        "\n",
        "B = newR.copy().T\n",
        "sums = B.sum(axis=0,keepdims=1)\n",
        "#print(sums)\n",
        "sums[sums==0] = 1\n",
        "B = B / sums\n",
        "print(B)\n",
        "\n",
        "for i in range(1,100):\n",
        "    drnew = alpha * np.dot(B, pr) # + (1-alpha) * all_zeros ### UPDATED IMPORTANCE OF ORIG FEATURES\n",
        "    prnew = alpha * np.dot(A, dr) + (1-alpha) * fimpr       ### UPDATED IMPORTANCE OF PROJ FEATURES\n",
        "    denom = (drnew.sum()+prnew.sum())\n",
        "    #print(denom) # should be 1\n",
        "    dr = drnew.reshape(R_ohe.shape[1],1) / denom\n",
        "    pr = prnew.reshape(R_ohe.shape[0],1) / denom\n",
        "    #print()\n",
        "\n",
        "print(dr/dr.sum())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2cBjlznR55S",
        "outputId": "129317ad-6255-4f94-daba-4288cd81ee87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of original features  8\n",
            "Number of projected features  10\n",
            "[[ 0.69958179]\n",
            " [-0.72608848]\n",
            " [-0.59186913]\n",
            " [-0.29578369]\n",
            " [-0.10775036]\n",
            " [-0.29170927]\n",
            " [ 0.34287704]\n",
            " [ 0.91669548]\n",
            " [-1.13578369]\n",
            " [ 0.06929278]]\n",
            "[[0.33333333 0.33333333 0.33333333 0.         0.         0.5        0.2        0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.2        0.         0.         1.        ]\n",
            " [0.33333333 0.         0.         0.         0.33333333 0.         0.         1.         0.         0.        ]\n",
            " [0.         0.         0.         0.5        0.         0.         0.         0.         0.         0.        ]\n",
            " [0.33333333 0.33333333 0.33333333 0.         0.33333333 0.         0.2        0.         0.         0.        ]\n",
            " [0.         0.33333333 0.33333333 0.         0.         0.5        0.2        0.         0.         0.        ]\n",
            " [0.         0.         0.         0.5        0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.33333333 0.         0.2        0.         0.         0.        ]]\n",
            "[[-0.34451921]\n",
            " [ 0.03959557]\n",
            " [ 0.78694764]\n",
            " [-0.08617408]\n",
            " [-0.12458693]\n",
            " [-0.47680526]\n",
            " [-0.08617408]\n",
            " [ 0.05229731]]\n"
          ]
        }
      ],
      "source": [
        "# attribution to original features\n",
        "# 2. BY HITS\n",
        "from numpy import linalg as LA\n",
        "\n",
        "print(\"Number of original features \", R_ohe.shape[1])\n",
        "print(\"Number of projected features \", R_ohe.shape[0])\n",
        "alpha = .5\n",
        "\n",
        "newR = R_ohe.copy()\n",
        "\n",
        "#Option 0: Use with BOTH + & -\n",
        "fimpr = fimportance3.copy() # which one to use\n",
        "\n",
        "#Option 1: # Ignore negative-importance (projection) features\n",
        "#indices = np.array(np.where(fimpr < 0))\n",
        "          #Remove associated edges\n",
        "#print(indices)\n",
        "#newR[indices[0],:] =0 \n",
        "#print(newR.T)\n",
        "#fimpr[fimpr < 0] = 0 \n",
        "\n",
        "#Option 2: # Shift so that minimum is zero\n",
        "#if np.min(fimpr) < 0:\n",
        "#  fimpr = fimpr + np.abs(np.min(fimpr))   \n",
        "\n",
        "\n",
        "#fimpr = fimpr / LA.norm(fimpr,2)\n",
        "\n",
        "fimpr = fimpr.reshape(R_ohe.shape[0],1)\n",
        "pr = fimpr.copy()\n",
        "print(pr)\n",
        "#print(fimpr.sum())\n",
        "\n",
        "B = newR.T\n",
        "sums = B.sum(axis=0,keepdims=1)\n",
        "#print(sums)\n",
        "sums[sums==0] = 1\n",
        "B = B / sums\n",
        "print(B)\n",
        "\n",
        "for i in range(1,3):\n",
        "    #dr = np.dot(R_ohe.T, pr)\n",
        "    dr = np.dot(B, pr)\n",
        "    ## normalize dr:\n",
        "    #print(dr.sum())\n",
        "    dr = dr.reshape(newR.shape[1],1) / LA.norm(dr,2) #dr.sum()\n",
        "    #print(dr)\n",
        "    pr = np.dot(newR, dr)\n",
        "    ## normalize pr:\n",
        "    #print(pr.sum())\n",
        "    \n",
        "    pr = alpha * pr + (1-alpha) * fimpr\n",
        "    pr = pr.reshape(newR.shape[0],1) / LA.norm(pr,2) #pr.sum()  \n",
        "    #print(fimpr)\n",
        "     \n",
        "print(dr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4EaXIhRgJS0",
        "outputId": "0b0e05c3-2994-44b8-fd28-962b445f41f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.23319393 -0.24202949 -0.19728971 -0.         -0.         -0.14585463  0.06857541  0.         -0.          0.        ]\n",
            " [ 0.         -0.         -0.         -0.         -0.         -0.          0.06857541  0.         -0.          0.06929278]\n",
            " [ 0.23319393 -0.         -0.         -0.         -0.03591679 -0.          0.          0.91669548 -0.          0.        ]\n",
            " [ 0.         -0.         -0.         -0.14789184 -0.         -0.          0.          0.         -0.          0.        ]\n",
            " [ 0.23319393 -0.24202949 -0.19728971 -0.         -0.03591679 -0.          0.06857541  0.         -0.          0.        ]\n",
            " [ 0.         -0.24202949 -0.19728971 -0.         -0.         -0.14585463  0.06857541  0.         -0.          0.        ]\n",
            " [ 0.         -0.         -0.         -0.14789184 -0.         -0.          0.          0.         -0.          0.        ]\n",
            " [ 0.         -0.         -0.         -0.         -0.03591679 -0.          0.06857541  0.         -0.          0.        ]]\n",
            "[[ 9.97925047e-02]\n",
            " [ 1.20221168e-03]\n",
            " [ 9.89720394e-01]\n",
            " [-2.82646698e-25]\n",
            " [ 9.83331014e-02]\n",
            " [ 2.85763156e-02]\n",
            " [-2.82646698e-25]\n",
            " [ 2.74725027e-03]]\n"
          ]
        }
      ],
      "source": [
        "# attribution to original features\n",
        "# 3. BY SIGNED HITS\n",
        "\n",
        "epsilon = 0.1\n",
        "\n",
        "dr = np.ones(R_ohe.shape[1]) * epsilon # importance\n",
        "pr = np.ones(R_ohe.shape[0]) * epsilon # authority/accurateness\n",
        "#print(pr)\n",
        "\n",
        "B = R_ohe.copy().T\n",
        "sums = B.sum(axis=0,keepdims=1)\n",
        "#print(sums)\n",
        "sums[sums==0] = 1\n",
        "scaledRtrans = scaledR.T / sums\n",
        "print(scaledRtrans)\n",
        "\n",
        "for i in range(1,20):\n",
        "    #dr = np.dot(R_ohe.T, pr)\n",
        "    dr = np.dot(scaledRtrans, pr)\n",
        "    #newdr = np.array([1 - (1 / (1 + np.exp(x)))  for x in dr])\n",
        "    #dr = dr + np.abs(np.min(dr))  \n",
        "    #dr = np.array(alpha * dr.T + (1-alpha) * np.ones(R_ohe.shape[1]) / R_ohe.shape[1])\n",
        "    ## normalize dr:\n",
        "    dr = dr.reshape(scaledR.shape[1],1) / LA.norm(dr,2) # abs(dr.sum())\n",
        "    #print(dr)\n",
        "\n",
        "    pr = np.dot(scaledRtrans.T, dr)\n",
        "    ## normalize pr:\n",
        "    #print(pr.sum())\n",
        "    #pr = pr + np.abs(np.min(pr))  \n",
        "    \n",
        "    pr = pr.reshape(scaledR.shape[0],1) / LA.norm(pr,2) # abs(pr.sum())\n",
        "    #print(fimpr)\n",
        "     \n",
        "print(dr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rww-N2e3wAx8"
      },
      "source": [
        "## Running options"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WprZspQoaaA2"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSUKVxwMYVfK"
      },
      "outputs": [],
      "source": [
        "def run_xstream_all():\n",
        "    all_outputs = []\n",
        "    min_max = True\n",
        "    if min_max:\n",
        "        print(\"Min-max normalize\")\n",
        "        all_outputs.append(\"Min-max normalize\")\n",
        "        start = time.time()\n",
        "        normDF = min_max_normalize(myDF) #100+ columns\n",
        "        normDF.show()\n",
        "        end = time.time()\n",
        "        print(\"Time = %.3fs\" % (end-start))\n",
        "        all_outputs.append(\"Time = %.3fs\" % (end-start))\n",
        "    else:\n",
        "        normDF = myDF\n",
        "    \n",
        "    print(\"Projection\")\n",
        "    all_outputs.append(\"Projection\")\n",
        "    start = time.time()\n",
        "    if projection:\n",
        "        pDF = projectDF(normDF, projdim, sc=sc)\n",
        "    else:\n",
        "        pDF = normDF\n",
        "    deltamax = feature_range(pDF)\n",
        "    print(deltamax)\n",
        "    # convert into pair rdd\n",
        "    pDF_pair = pDF.rdd.map(lambda x: (list(x[0:-1]), x[-1]))\n",
        "    pDF_pair.cache()\n",
        "    end = time.time()\n",
        "    print(\"Time = %.3fs\" % (end-start))\n",
        "    all_outputs.append(\"Time = %.3fs\" % (end-start))\n",
        "    \n",
        "    print(\"Fitting\")\n",
        "    all_outputs.append(\"Fitting\")\n",
        "    start = time.time()\n",
        "    cf = Chains(deltamax, nchains=nchains, depth=depth, nthreads_fit=nthreads_fit, nthreads_score=nthreads_score, samplerate=samplerate)\n",
        "    cf = cf.fitparallel(pDF_pair)\n",
        "    # saving cf\n",
        "    # pickle.dump(cf, open(model_file, \"wb\"))\n",
        "    end = time.time()\n",
        "    print(\"Time = %.3fs\" % (end-start))\n",
        "    all_outputs.append(\"Time = %.3fs\" % (end-start))\n",
        "    \n",
        "    print(\"Scoring\")\n",
        "    all_outputs.append(\"Scoring\")\n",
        "    start = time.time()\n",
        "    #anomalyScoresRDD = pDF_pair.map(lambda x: (float(cf.score(x[0])), float(x[1])))\n",
        "    anomalyScoresRDD = cf.scoreparallel_test(pDF_pair)\n",
        "    \n",
        "    metrics = BinaryClassificationMetrics(anomalyScoresRDD)\n",
        "    auc = metrics.areaUnderROC\n",
        "    ap = metrics.areaUnderPR\n",
        "    \n",
        "    end = time.time()\n",
        "    print(\"Time = %.3fs\" % (end-start))\n",
        "    all_outputs.append(\"Time = %.3fs\" % (end-start))\n",
        "    \n",
        "    print(\"xstream: AUC =\", auc)\n",
        "    all_outputs.append(\"xstream: AUC = %.5f\" % auc)\n",
        "    \n",
        "    print(\"xstream: AP =\", ap)\n",
        "    all_outputs.append(\"xstream: AP = %.5f\" % ap)\n",
        "    \n",
        "    print(\"Printing\")\n",
        "    all_outputs.append(\"Printing\")\n",
        "    start = time.time()\n",
        "    \n",
        "    anomalyScoresRDD.toDF().repartition(1).write.format('com.databricks.spark.csv').save(\"/ocean/projects/cie170025p/shared/SpamURL_scores/\" + score_folder, header = 'false')\n",
        "    \n",
        "    end = time.time()\n",
        "    print(\"Time = %.3fs\" % (end-start))\n",
        "    all_outputs.append(\"Time = %.3fs\" % (end-start))\n",
        "    \n",
        "    with open(output_file, 'w') as f:\n",
        "        for s in all_outputs:\n",
        "            print(s, file=f)\n",
        "\n",
        "def run_xstream_fit():\n",
        "    all_outputs = []\n",
        "    min_max = False\n",
        "    if min_max:\n",
        "        print(\"Min-max normalize\")\n",
        "        all_outputs.append(\"Min-max normalize\")\n",
        "        start = time.time()\n",
        "        normDF = min_max_normalize(myDF) #100+ columns\n",
        "        normDF.show()\n",
        "        end = time.time()\n",
        "        print(\"Time = %.3fs\" % (end-start))\n",
        "        all_outputs.append(\"Time = %.3fs\" % (end-start))\n",
        "    else:\n",
        "        normDF = myDF\n",
        "    \n",
        "    print(\"Projection\")\n",
        "    all_outputs.append(\"Projection\")\n",
        "    start = time.time()\n",
        "    if projection:\n",
        "        pDF = projectDF(normDF, projdim, sc=sc)\n",
        "    else:\n",
        "        pDF = normDF\n",
        "    deltamax = feature_range(pDF)\n",
        "    print(deltamax)\n",
        "    # convert into pair rdd\n",
        "    pDF_pair = pDF.rdd.map(lambda x: (list(x[0:-1]), x[-1]))\n",
        "    pDF_pair.cache()\n",
        "    end = time.time()\n",
        "    print(\"Time = %.3fs\" % (end-start))\n",
        "    all_outputs.append(\"Time = %.3fs\" % (end-start))\n",
        "    \n",
        "    print(\"Fitting\")\n",
        "    all_outputs.append(\"Fitting\")\n",
        "    start = time.time()\n",
        "    cf = Chains(deltamax, nchains=nchains, depth=depth, nthreads_fit=nthreads_fit, nthreads_score=nthreads_score, samplerate=samplerate)\n",
        "    cf = cf.fitparallel(pDF_pair)\n",
        "    # saving cf\n",
        "    pickle.dump(cf, open(model_file, \"wb\"))\n",
        "    end = time.time()\n",
        "    print(\"Time = %.3fs\" % (end-start))\n",
        "    all_outputs.append(\"Time = %.3fs\" % (end-start))\n",
        "    \n",
        "    with open(output_file, 'w') as f:\n",
        "        for s in all_outputs:\n",
        "            print(s, file=f)\n",
        "\n",
        "def run_xstream_score_print():\n",
        "    all_outputs = []\n",
        "    min_max = False\n",
        "    if min_max:\n",
        "        print(\"Min-max normalize\")\n",
        "        all_outputs.append(\"Min-max normalize\")\n",
        "        start = time.time()\n",
        "        normDF = min_max_normalize(myDF) #100+ columns\n",
        "        normDF.show()\n",
        "        end = time.time()\n",
        "        print(\"Time = %.3fs\" % (end-start))\n",
        "        all_outputs.append(\"Time = %.3fs\" % (end-start))\n",
        "    else:\n",
        "        normDF = myDF\n",
        "    \n",
        "    print(\"Projection\")\n",
        "    all_outputs.append(\"Projection\")\n",
        "    start = time.time()\n",
        "    if projection:\n",
        "        pDF = projectDF(normDF, projdim, sc=sc)\n",
        "    else:\n",
        "        pDF = normDF\n",
        "    deltamax = feature_range(pDF)\n",
        "    print(deltamax)\n",
        "    # convert into pair rdd\n",
        "    pDF_pair = pDF.rdd.map(lambda x: (list(x[0:-1]), x[-1]))\n",
        "    pDF_pair.cache()\n",
        "    end = time.time()\n",
        "    print(\"Time = %.3fs\" % (end-start))\n",
        "    all_outputs.append(\"Time = %.3fs\" % (end-start))\n",
        "    \n",
        "    cf = pickle.load(open(model_file, \"rb\"))\n",
        "    print(\"Scoring\")\n",
        "    all_outputs.append(\"Scoring\")\n",
        "    start = time.time()\n",
        "    #anomalyScoresRDD = pDF_pair.map(lambda x: (float(cf.score(x[0])), float(x[1])))\n",
        "    anomalyScoresRDD = cf.scoreparallel_test(pDF_pair)    \n",
        "    \n",
        "    end = time.time()\n",
        "    print(\"Time = %.3fs\" % (end-start))\n",
        "    all_outputs.append(\"Time = %.3fs\" % (end-start))\n",
        "    \n",
        "    print(\"Printing\")\n",
        "    all_outputs.append(\"Printing\")\n",
        "    start = time.time()\n",
        "    \n",
        "    anomalyScoresRDD.toDF().repartition(1).write.format('com.databricks.spark.csv').save(\"/ocean/projects/cie170025p/shared/\" + score_folder, header = 'false')\n",
        "    #pickle.dump(anomalyScores, open(\"/ocean/projects/cie170025p/shared/score_\" + str(part) + \".pkl\", \"wb\"))\n",
        "    \n",
        "    end = time.time()\n",
        "    print(\"Time = %.3fs\" % (end-start))\n",
        "    all_outputs.append(\"Time = %.3fs\" % (end-start))\n",
        "\n",
        "    with open(output_file, 'w') as f:\n",
        "        for s in all_outputs:\n",
        "            print(s, file=f)\n",
        "            \n",
        "def run_xstream_score_all():\n",
        "    all_outputs = []\n",
        "    min_max = False\n",
        "    if min_max:\n",
        "        print(\"Min-max normalize\")\n",
        "        all_outputs.append(\"Min-max normalize\")\n",
        "        start = time.time()\n",
        "        normDF = min_max_normalize(myDF) #100+ columns\n",
        "        normDF.show()\n",
        "        end = time.time()\n",
        "        print(\"Time = %.3fs\" % (end-start))\n",
        "        all_outputs.append(\"Time = %.3fs\" % (end-start))\n",
        "    else:\n",
        "        normDF = myDF\n",
        "    \n",
        "    print(\"Projection\")\n",
        "    all_outputs.append(\"Projection\")\n",
        "    start = time.time()\n",
        "    if projection:\n",
        "        pDF = projectDF(normDF, projdim, sc=sc)\n",
        "    else:\n",
        "        pDF = normDF\n",
        "    deltamax = feature_range(pDF)\n",
        "    print(deltamax)\n",
        "    # convert into pair rdd\n",
        "    pDF_pair = pDF.rdd.map(lambda x: (list(x[0:-1]), x[-1]))\n",
        "    pDF_pair.cache()\n",
        "    end = time.time()\n",
        "    print(\"Time = %.3fs\" % (end-start))\n",
        "    all_outputs.append(\"Time = %.3fs\" % (end-start))\n",
        "    \n",
        "    cf = pickle.load(open(model_file, \"rb\"))\n",
        "    print(\"Scoring\")\n",
        "    all_outputs.append(\"Scoring\")\n",
        "    start = time.time()\n",
        "    #anomalyScoresRDD = pDF_pair.map(lambda x: (float(cf.score(x[0])), float(x[1])))\n",
        "    anomalyScoresRDD = cf.scoreparallel_test(pDF_pair)\n",
        "    \n",
        "    metrics = BinaryClassificationMetrics(anomalyScoresRDD)\n",
        "    auc = metrics.areaUnderROC\n",
        "    ap = metrics.areaUnderPR\n",
        "    \n",
        "    end = time.time()\n",
        "    print(\"Time = %.3fs\" % (end-start))\n",
        "    all_outputs.append(\"Time = %.3fs\" % (end-start))\n",
        "    \n",
        "    print(\"xstream: AUC =\", auc)\n",
        "    all_outputs.append(\"xstream: AUC = %.5f\" % auc)\n",
        "    \n",
        "    print(\"xstream: AP =\", ap)\n",
        "    all_outputs.append(\"xstream: AP = %.5f\" % ap)\n",
        "    \n",
        "def run_xstream_score_combine():\n",
        "    all_outputs = []\n",
        "        \n",
        "    print(\"Loading\")\n",
        "    all_outputs.append(\"Loading\")\n",
        "    start = time.time()\n",
        "\n",
        "    score_folder = \"score_\" + str(nchains) + \"_\" + str(depth) + \"_1.csv\"\n",
        "    scores_1 = pd.read_csv(\"/ocean/projects/cie170025p/shared/\" + score_folder + \"/score.csv\", header=None)\n",
        "    score_folder = \"score_\" + str(nchains) + \"_\" + str(depth) + \"_2.csv\"\n",
        "    scores_2 = pd.read_csv(\"/ocean/projects/cie170025p/shared/\" + score_folder + \"/score.csv\", header=None)\n",
        "    score_folder = \"score_\" + str(nchains) + \"_\" + str(depth) + \"_3.csv\"\n",
        "    scores_3 = pd.read_csv(\"/ocean/projects/cie170025p/shared/\" + score_folder + \"/score.csv\", header=None)\n",
        "    score_folder = \"score_\" + str(nchains) + \"_\" + str(depth) + \"_4.csv\"\n",
        "    scores_4 = pd.read_csv(\"/ocean/projects/cie170025p/shared/\" + score_folder + \"/score.csv\", header=None)\n",
        "    \n",
        "    all_scores = pd.concat([scores_1, scores_2, scores_3, scores_4])\n",
        "    \n",
        "    del scores_1\n",
        "    del scores_2\n",
        "    del scores_3\n",
        "    del scores_4\n",
        "    \n",
        "    end = time.time()\n",
        "    print(\"Time = %.3fs\" % (end-start))\n",
        "    all_outputs.append(\"Time = %.3fs\" % (end-start))\n",
        "    \n",
        "    print(\"Scoring\")\n",
        "    all_outputs.append(\"Scoring\")\n",
        "    start = time.time()\n",
        "    \n",
        "    y_score = all_scores[0].to_numpy()\n",
        "    y_true = all_scores[1].to_numpy()\n",
        "    \n",
        "    auc = roc_auc_score(y_true, y_score)\n",
        "    ap = average_precision_score(y_true, y_score)\n",
        "    \n",
        "    print(\"xstream: AUC =\", auc)\n",
        "    all_outputs.append(\"xstream: AUC = %.5f\" % auc)\n",
        "    \n",
        "    print(\"xstream: AP =\", ap)\n",
        "    all_outputs.append(\"xstream: AP = %.5f\" % ap)\n",
        "    \n",
        "    end = time.time()\n",
        "    print(\"Time = %.3fs\" % (end-start))\n",
        "    all_outputs.append(\"Time = %.3fs\" % (end-start))\n",
        "    \n",
        "    print(\"Scoring F1\")\n",
        "    all_outputs.append(\"Scoring F1\")\n",
        "    start = time.time()\n",
        "    \n",
        "    all_scores.sort_values(by=[0], inplace=True, ascending=False)\n",
        "\n",
        "    num_outliers = 1000000\n",
        "    num_selected = 1000000 # 500000, 1000000, 2000000\n",
        "    tp = 0\n",
        "    fp = 0\n",
        "    for i in range(num_selected):\n",
        "        if all_scores.iloc[i, 1] == 1:\n",
        "            tp += 1\n",
        "        else:\n",
        "            fp += 1\n",
        "\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / num_outliers\n",
        "    f1 = 2 * precision * recall / (precision + recall)\n",
        "    \n",
        "    print(\"xstream: F1 =\", f1)\n",
        "    all_outputs.append(\"xstream: F1 = %.5f\" % f1)\n",
        "    \n",
        "    end = time.time()\n",
        "    print(\"Time = %.3fs\" % (end-start))\n",
        "    all_outputs.append(\"Time = %.3fs\" % (end-start))\n",
        "    \n",
        "    with open(output_file, 'w') as f:\n",
        "        for s in all_outputs:\n",
        "            print(s, file=f)\n",
        "            \n",
        "def run_xstream_score_combine_10():\n",
        "    all_outputs = []\n",
        "        \n",
        "    print(\"Loading\")\n",
        "    all_outputs.append(\"Loading\")\n",
        "    start = time.time()\n",
        "\n",
        "    score_folder = \"score_\" + str(nchains) + \"_\" + str(depth) + \"_1.csv\"\n",
        "    scores_1 = pd.read_csv(\"/ocean/projects/cie170025p/shared/\" + score_folder + \"/score.csv\", header=None)\n",
        "    score_folder = \"score_\" + str(nchains) + \"_\" + str(depth) + \"_2.csv\"\n",
        "    scores_2 = pd.read_csv(\"/ocean/projects/cie170025p/shared/\" + score_folder + \"/score.csv\", header=None)\n",
        "    score_folder = \"score_\" + str(nchains) + \"_\" + str(depth) + \"_3.csv\"\n",
        "    scores_3 = pd.read_csv(\"/ocean/projects/cie170025p/shared/\" + score_folder + \"/score.csv\", header=None)\n",
        "    score_folder = \"score_\" + str(nchains) + \"_\" + str(depth) + \"_4.csv\"\n",
        "    scores_4 = pd.read_csv(\"/ocean/projects/cie170025p/shared/\" + score_folder + \"/score.csv\", header=None)\n",
        "    score_folder = \"score_\" + str(nchains) + \"_\" + str(depth) + \"_5.csv\"\n",
        "    scores_5 = pd.read_csv(\"/ocean/projects/cie170025p/shared/\" + score_folder + \"/score.csv\", header=None)\n",
        "    score_folder = \"score_\" + str(nchains) + \"_\" + str(depth) + \"_6.csv\"\n",
        "    scores_6 = pd.read_csv(\"/ocean/projects/cie170025p/shared/\" + score_folder + \"/score.csv\", header=None)\n",
        "    score_folder = \"score_\" + str(nchains) + \"_\" + str(depth) + \"_7.csv\"\n",
        "    scores_7 = pd.read_csv(\"/ocean/projects/cie170025p/shared/\" + score_folder + \"/score.csv\", header=None)\n",
        "    score_folder = \"score_\" + str(nchains) + \"_\" + str(depth) + \"_8.csv\"\n",
        "    scores_8 = pd.read_csv(\"/ocean/projects/cie170025p/shared/\" + score_folder + \"/score.csv\", header=None)\n",
        "    score_folder = \"score_\" + str(nchains) + \"_\" + str(depth) + \"_9.csv\"\n",
        "    scores_9 = pd.read_csv(\"/ocean/projects/cie170025p/shared/\" + score_folder + \"/score.csv\", header=None)\n",
        "    score_folder = \"score_\" + str(nchains) + \"_\" + str(depth) + \"_10.csv\"\n",
        "    scores_10 = pd.read_csv(\"/ocean/projects/cie170025p/shared/\" + score_folder + \"/score.csv\", header=None)\n",
        "    \n",
        "    all_scores = pd.concat([scores_1, scores_2, scores_3, scores_4, scores_5, scores_6, scores_7, scores_8, scores_9, scores_10])\n",
        "    \n",
        "    del scores_1\n",
        "    del scores_2\n",
        "    del scores_3\n",
        "    del scores_4\n",
        "    del scores_5\n",
        "    del scores_6\n",
        "    del scores_7\n",
        "    del scores_8\n",
        "    del scores_9\n",
        "    del scores_10\n",
        "    \n",
        "    end = time.time()\n",
        "    print(\"Time = %.3fs\" % (end-start))\n",
        "    all_outputs.append(\"Time = %.3fs\" % (end-start))\n",
        "    \n",
        "    print(\"Scoring\")\n",
        "    all_outputs.append(\"Scoring\")\n",
        "    start = time.time()\n",
        "    \n",
        "    y_score = all_scores[0].to_numpy()\n",
        "    y_true = all_scores[1].to_numpy()\n",
        "    \n",
        "    auc = roc_auc_score(y_true, y_score)\n",
        "    ap = average_precision_score(y_true, y_score)\n",
        "    \n",
        "    print(\"xstream: AUC =\", auc)\n",
        "    all_outputs.append(\"xstream: AUC = %.5f\" % auc)\n",
        "    \n",
        "    print(\"xstream: AP =\", ap)\n",
        "    all_outputs.append(\"xstream: AP = %.5f\" % ap)\n",
        "    \n",
        "    end = time.time()\n",
        "    print(\"Time = %.3fs\" % (end-start))\n",
        "    all_outputs.append(\"Time = %.3fs\" % (end-start))\n",
        "    \n",
        "    print(\"Scoring F1\")\n",
        "    all_outputs.append(\"Scoring F1\")\n",
        "    start = time.time()\n",
        "    \n",
        "    all_scores.sort_values(by=[0], inplace=True, ascending=False)\n",
        "\n",
        "    num_outliers = 1000000\n",
        "    num_selected = 1000000 # 500000, 1000000, 2000000\n",
        "    tp = 0\n",
        "    fp = 0\n",
        "    for i in range(num_selected):\n",
        "        if all_scores.iloc[i, 1] == 1:\n",
        "            tp += 1\n",
        "        else:\n",
        "            fp += 1\n",
        "\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / num_outliers\n",
        "    f1 = 2 * precision * recall / (precision + recall)\n",
        "    \n",
        "    print(\"xstream: F1 =\", f1)\n",
        "    all_outputs.append(\"xstream: F1 = %.5f\" % f1)\n",
        "    \n",
        "    end = time.time()\n",
        "    print(\"Time = %.3fs\" % (end-start))\n",
        "    all_outputs.append(\"Time = %.3fs\" % (end-start))\n",
        "\n",
        "    with open(output_file, 'w') as f:\n",
        "        for s in all_outputs:\n",
        "            print(s, file=f)\n",
        "            \n",
        "def run_xstream_score_f1():\n",
        "    all_outputs = []\n",
        "        \n",
        "    print(\"Loading\")\n",
        "    all_outputs.append(\"Loading\")\n",
        "    start = time.time()\n",
        "\n",
        "    all_scores = pd.read_csv(\"/ocean/projects/cie170025p/shared/SpamURL_scores/\" + score_folder + \"/score.csv\", header=None)\n",
        "    \n",
        "    end = time.time()\n",
        "    print(\"Time = %.3fs\" % (end-start))\n",
        "    all_outputs.append(\"Time = %.3fs\" % (end-start))\n",
        "    \n",
        "    print(\"Scoring F1\")\n",
        "    all_outputs.append(\"Scoring F1\")\n",
        "    start = time.time()\n",
        "    \n",
        "    all_scores.sort_values(by=[0], inplace=True, ascending=False)\n",
        "\n",
        "    num_outliers = 792145\n",
        "    num_selected = 792145\n",
        "    tp = 0\n",
        "    fp = 0\n",
        "    for i in range(num_selected):\n",
        "        if all_scores.iloc[i, 1] == 1:\n",
        "            tp += 1\n",
        "        else:\n",
        "            fp += 1\n",
        "\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / num_outliers\n",
        "    f1 = 2 * precision * recall / (precision + recall)\n",
        "    \n",
        "    print(\"xstream: F1 =\", f1)\n",
        "    all_outputs.append(\"xstream: F1 = %.5f\" % f1)\n",
        "    \n",
        "    end = time.time()\n",
        "    print(\"Time = %.3fs\" % (end-start))\n",
        "    all_outputs.append(\"Time = %.3fs\" % (end-start))\n",
        "    \n",
        "    with open(output_file, 'w') as f:\n",
        "        for s in all_outputs:\n",
        "            print(s, file=f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G7FExLcKYsnW",
        "outputId": "379d1d85-efcf-4f14-b3ac-267b0918d670"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Projection\n"
          ]
        },
        {
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-6b8ef15fec53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#run_xstream_score_combine()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mrun_xstream_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-39-9fe27786f1c6>\u001b[0m in \u001b[0;36mrun_xstream_all\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprojection\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mpDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprojectDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormDF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprojdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mpDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-6316289c7810>\u001b[0m in \u001b[0;36mprojectDF\u001b[0;34m(df, projdim, sc)\u001b[0m\n\u001b[1;32m      9\u001b[0m                                      \u001b[0mmutable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                      sc=sc)\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mprojectedDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprojector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprojectedDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mtoDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \"\"\"\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    673\u001b[0m             return super(SparkSession, self).createDataFrame(\n\u001b[1;32m    674\u001b[0m                 data, schema, samplingRatio, verifySchema)\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    484\u001b[0m         \"\"\"\n\u001b[1;32m    485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStructType\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \"\"\"\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m             raise ValueError(\"The first row in RDD is empty, \"\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1585\u001b[0m         \"\"\"\n\u001b[0;32m-> 1586\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1587\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1588\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1231\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1234\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 43.0 failed 1 times, most recent failure: Lost task 0.0 in stage 43.0 (TID 106) (424dc727712d executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/content/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/content/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/content/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/content/spark-3.1.2-bin-hadoop2.7/python/pyspark/rdd.py\", line 1560, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/content/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-34-6316289c7810>\", line 11, in <lambda>\n  File \"<ipython-input-33-f380d0bfb854>\", line 35, in fit_transform\n  File \"<__array_function__ internals>\", line 6, in dot\nValueError: data type must provide an itemsize\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/content/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/content/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/content/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/content/spark-3.1.2-bin-hadoop2.7/python/pyspark/rdd.py\", line 1560, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/content/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-34-6316289c7810>\", line 11, in <lambda>\n  File \"<ipython-input-33-f380d0bfb854>\", line 35, in fit_transform\n  File \"<__array_function__ internals>\", line 6, in dot\nValueError: data type must provide an itemsize\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
          ]
        }
      ],
      "source": [
        "#from memory_profiler import memory_usage\n",
        "memory = False\n",
        "if memory:\n",
        "    print(\"Testing memory usage\")\n",
        "    usage = memory_usage(proc=run_xstream_all)\n",
        "    #usage = memory_usage(proc=run_xstream_score_f1)\n",
        "    #usage = memory_usage(proc=run_xstream_fit)\n",
        "    #usage = memory_usage(proc=run_xstream_score_print)\n",
        "    #usage = memory_usage(proc=run_xstream_score_combine_10)\n",
        "    print(\"Peak memory = %.3fM\" % max(usage))\n",
        "    print(\"Average memory = %.3fM\" % (sum(usage)/len(usage)))\n",
        "    with open(output_file, 'a') as f:\n",
        "        print(\"Peak memory = %.3fM\" % max(usage), file=f)\n",
        "        print(\"Average memory = %.3fM\" % (sum(usage)/len(usage)), file=f)\n",
        "else:\n",
        "    #run_xstream_score_combine()\n",
        "    run_xstream_all()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Rww-N2e3wAx8"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}